#########################################################
########################################################
## DESCRIPTION
## multiple regression with categorical explanatory
#  SE for linear combination of betahats
## ENDDESCRIPTION
#########################################################

#########################################################
## KEYWORDS('statistics', 'regression','covariance matrix of estimator')
#########################################################

#########################################################
## DBsubject('Statistics')
## Date('2017/02/15')
## Author('H Joe')
## Institution('UBC')
## TitleText1('')
## EditionText1('')
## AuthorText1('')
## Section1('')
## Problem1('')

#########################################################

## Initializations:
DOCUMENT();

loadMacros(
  "PGstandard.pl",
  "PGchoicemacros.pl",
  "parserRadioButtons.pl",
  "MathObjects.pl",
  "parserMultiAnswer.pl",
  "answerHints.pl",
  "regrfnsPG.pl",   # functions for regression and sampling
);

Context("Matrix");

@rw0=(  0.333470,-0.001230,  0.000379, -0.006350,-0.007631,-0.000744, -0.036518,-0.054614, -0.049256, -0.043163);
@rw1=( -0.001230, 0.000011,  0.000003,  0.000000, 0.000036, 0.000001, 0.000019, 0.000121,  0.000200,  0.000099);
@rw2=(  0.000379, 0.000003,  0.000019, -0.000035, 0.000026,-0.000011, -0.000072, 0.000012,  0.000032, -0.000016);
@rw3=( -0.006350, 0.000000, -0.000035,  0.000281, 0.000077, 0.000019, 0.000796, 0.000944,  0.000292,  0.000309);
@rw4=( -0.007631, 0.000036,  0.000026,  0.000077, 0.000510,-0.000040, 0.001221, 0.002007,  0.001827,  0.001396);
@rw5=( -0.000744, 0.000001, -0.000011,  0.000019,-0.000040, 0.000016, -0.000032,-0.000069, -0.000021,  0.000068);
@rw6=( -0.036518, 0.000019, -0.000072,  0.000796, 0.001221,-0.000032, 0.022795, 0.016794,  0.014729,  0.014576);
@rw7=( -0.054614, 0.000121,  0.000012,  0.000944, 0.002007,-0.000069, 0.016794, 0.030437,  0.017899,  0.016878);
@rw8=( -0.049256, 0.000200,  0.000032,  0.000292, 0.001827,-0.000021, 0.014729, 0.017899,  0.025177,  0.017092);
@rw9=( -0.043163, 0.000099, -0.000016,  0.000309, 0.001396, 0.000068, 0.014576, 0.016878,  0.017092,  0.029926);

$acov=Matrix ([@rw0],[@rw1],[@rw2],[@rw3],[@rw4],[@rw5],[@rw6],[@rw7],[@rw8],[@rw9]); 

$m=4;
@ii=isamp($m,$m);
$i1=$ii[0]+6;
$i2=$ii[1]+6;
$i3=$ii[2]+6;
$i4=$ii[3]+6;

$str="acov=matrix(c(";
for($i=0;$i<=8;$i++) { $str=$str . $rw0[$i] . ", "; }
$str=$str . $rw0[9] . ",  ";
for($i=0;$i<=8;$i++) { $str=$str . $rw1[$i] . ", "; }
$str=$str . $rw1[9] . ",  ";
for($i=0;$i<=8;$i++) { $str=$str . $rw2[$i] . ", "; }
$str=$str . $rw2[9] . ",  ";
for($i=0;$i<=8;$i++) { $str=$str . $rw3[$i] . ", "; }
$str=$str . $rw3[9] . ",  ";
for($i=0;$i<=8;$i++) { $str=$str . $rw4[$i] . ", "; }
$str=$str . $rw4[9] . ",  ";
for($i=0;$i<=8;$i++) { $str=$str . $rw5[$i] . ", "; }
$str=$str . $rw5[9] . ",  ";
for($i=0;$i<=8;$i++) { $str=$str . $rw6[$i] . ", "; }
$str=$str . $rw6[9] . ",  ";
for($i=0;$i<=8;$i++) { $str=$str . $rw7[$i] . ", "; }
$str=$str . $rw7[9] . ",  ";
for($i=0;$i<=8;$i++) { $str=$str . $rw8[$i] . ", "; }
$str=$str . $rw8[9] . ",  ";
for($i=0;$i<=8;$i++) { $str=$str . $rw9[$i] . ", "; }
$str=$str . $rw9[9] . "),10,10)";


# answers

$tem=$acov->element($i1+1,$i1+1) + $acov->element($i2+1,$i2+1)
-2*$acov->element($i1+1,$i2+1);
$ansa=sqrt($tem);

$tem2=$acov->element($i1+1,$i1+1) + $acov->element($i2+1,$i2+1)
+2*$acov->element($i1+1,$i2+1)
 + 4*$acov->element($i3+1,$i3+1) - 4*$acov->element($i1+1,$i3+1) -
4*$acov->element($i2+1,$i3+1);
$ansb=sqrt($tem2);

$cov11=$acov->element($i1+1,$i1+1);
$cov22=$acov->element($i2+1,$i2+1);
$cov12=$acov->element($i1+1,$i2+1);
$cov33=$acov->element($i3+1,$i3+1);
$cov13=$acov->element($i1+1,$i3+1);
$cov23=$acov->element($i2+1,$i3+1);

Context()->texStrings;

#$i1, $i2, $i3, $i4
#$ansa, $ansb

BEGIN_TEXT


You are being given the covariance matrix of a multiple regression 
with \({\hat\beta}_0,{\hat\beta}_1,\ldots,{\hat\beta}_5,{\hat\beta}_6,
\ldots,{\hat\beta}_9\) where (compare university admissions data set
in previous WebWork homework set) \(x_1,\ldots,x_5\) are numerical
explanatory variables and
\(x_6,\ldots,x_9\) are 4 binary dummy variables for 4 different categories
relative to the baseline category.
The categorical explanatory variable has 5 categories.

$BR
$BR
You are given that the estimated covariance matrix is:
$BCENTER
  \($acov\) 
$ECENTER
or in R:
$BR
$str

$BR
$BR
Please use 3 decimal places for the answers below which are not
integer-valued.
$BR
$BR

$BR
$BBOLD Part a) $EBOLD
$BR
Based on the (estimated) covariance matrix,
what is the SE of \({\hat\beta}_{$i1} - {\hat\beta}_{$i2} \)?
Note that \({\hat\beta}_{$i1} - {\hat\beta}_{$i2} \) corresponds to
the estimated distance of the hyperplanes for two different categories
where the binary dummy variables are indexed as \(x_{$i1}\) and
\(x_{$i2}\).
$BR
\{ ans_rule(8) \} 
$BR

$BR
$BBOLD Part b) $EBOLD
$BR
Based on the (estimated) covariance matrix,
what is the SE of \({\hat\beta}_{$i1} + {\hat\beta}_{$i2} -2{\hat\beta}_{$i3}  \)?
Note that \({\hat\beta}_{$i1} + {\hat\beta}_{$i2}  -2{\hat\beta}_{$i3}\) corresponds to
a contrast to two categories (where the binary dummy variables are indexed
as \(x_{$i1}\)) and
\(x_{$i2}\) with a third category (where the binary dummy variable is
indexed as \(x_{$i3}\)).
$BR
\{ ans_rule(8) \} 
$BR

$BR

END_TEXT

#########################################################

BEGIN_HINT
Check the course pack, Sections 3.9 and 3.6
END_HINT

#########################################################
$showPartialCorrectAnswers = 1;
#########################################################

ANS( num_cmp($ansa,tol=> 0.0020, tolType=>"absolute") );
ANS( num_cmp($ansb,tol=> 0.0020, tolType=>"absolute") );

#########################################################
BEGIN_SOLUTION
$BR
$BR
(a) get
$cov11+ $cov22-2*$cov12
and then take a square root to get $ansa.
$BR
$BR
(b) get
$cov11+ $cov22+2*$cov12 + 4*$cov33 - 4*$cov13 -4*$cov23
and then take a square root to get
$ansb.
$BR
END_SOLUTION
#########################################################


ENDDOCUMENT();

