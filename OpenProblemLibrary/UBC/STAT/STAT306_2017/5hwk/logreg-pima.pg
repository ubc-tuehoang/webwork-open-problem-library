#########################################################
########################################################
## DESCRIPTION
## logistic regression and classification from holdout set
## ENDDESCRIPTION
#########################################################

#########################################################
#########################################################

#########################################################
## DBsubject(Statistics)
## DBchapter(Generalized linear methods)
## DBsection(Logistic regression)
## Date(2017/03/06)
## Institution(UBC)
## Author(H Joe)
## Level(4)
## KEYWORDS('statistics', 'logistic regression')
#########################################################
## Initializations:
DOCUMENT();

loadMacros(
  "PGstandard.pl",
  "PGchoicemacros.pl",
  "parserRadioButtons.pl",
  "MathObjects.pl",
  "parserMultiAnswer.pl",
  "answerHints.pl",
  "regrfnsPG.pl",   # functions for regression 
);

Context("Matrix");

sub logreg
{ local ($n,$i,$j,$k,$nn,$iter,$np,$np1,$phat,$ysum,$yy,$xx);
  local ($tem,$p,$q,$mx,$llk,$phat,$l0,$aic,$bic,$llr);
  local ($zeros,$difv,$sumv,$y,$xvec,$A,$b,$rhs,$seb);
  local ($xmat,$AA,$Amat,$rhsmat,$rhst,$LR,$dim,$soln,$B,$Ainv);
  
  my $n = shift @_;
  my $np = shift @_; # covariates , column of 1 not needed
  $np1=$np+1;
  @zeros=(0..$np);
  @A=(1..($np1*$np1));
  for($j=0;$j<=$np;$j++) { $zeros[$j]=0; }
  for($j=0;$j<$np1*$np1;$j++) { $A[$j]=0; }
  @difv=@zeros; @sumv=@zeros; @b=@zeros; @rhs=@zeros; @seb=@zeros;
  @y=@_[0..($n-1)];
  $nn=$#_;
  @xvec=@_[$n..($nn-$np1)];
  @b=@_[($nn-$np)..$nn];
  # create matrix for $xvec
  for($j=1;$j<=$np;$j++)
  { $i1=($j-1)*$n; $i2=$j*$n-1;
    if($j==1) { @xmat=[@xvec[$i1..$i2]]; }
    else { @xmat=(@xmat,[@xvec[$i1..$i2]]); }
  }

  $iter=0; $mx=1.;
  for($i=1,$ysum=0;$i<=$n;$i++) { $ysum+=$y[$i-1]; }
  $phat=$ysum/$n;
  $l0=$ysum*log($phat) + ($n-$ysum)*log(1.-$phat);
  for($j=0;$j<=$np;$j++) { $sumv[$j]=0.; }
  for($i=1;$i<=$n;$i++)
  { $yy=$y[$i-1];
    for($j=1;$j<=$np;$j++) { $sumv[$j]+=($xmat[$j-1][$i-1])*$yy; }
  }
  while($iter<30 && $mx>.000001)
  { for($j=0;$j<=$np;$j++)
    { for($k=0;$k<=$np;$k++) { $A[$j*$np1+$k]=0.; } } 
    $rhs[0]=$ysum; $llk=$b[0]*$ysum;
    for($j=1;$j<=$np;$j++) 
    { $rhs[$j]=$sumv[$j]; $llk+=$b[$j]*$sumv[$j]; }
    for($i=1;$i<=$n;$i++)
    { for($j=1,$tem=$b[0];$j<=$np;$j++) { $tem+=$b[$j]*($xmat[$j-1][$i-1]); }
      $q=1./(1.+exp($tem)); $p=1.-$q;
      $llk+=log($q);
      $rhs[0]-=$p; $A[0]-=$p*$q;
      for($j=1;$j<=$np;$j++)
      { $xx=$xmat[$j-1][$i-1];  $rhs[$j]-=$xx*$p;
        $A[$j]-=$xx*$p*$q; $A[$j*$np1]-=$xx*$p*$q;
        for($k=1;$k<=$np;$k++) 
        { $A[$j*$np1+$k]-=$xx*($xmat[$k-1][$i-1])*$p*$q; }
      }
    }
    for($j=1;$j<=$np1;$j++)
    { $i1=($j-1)*$np1; $i2=$j*$np1-1;
      if($j==1) { @Amat=[@A[$i1..$i2]]; }
      else { @Amat=(@Amat,[@A[$i1..$i2]]); }
    }
    $AA=Matrix(@Amat);
    $rhsmat=Matrix([@rhs]);
    $rhst=$rhsmat->transpose;  # cannot rewrite same variable
    $LR = $AA->decompose_LR();
    ($dim,$soln,$B) = $LR->solve_LR($rhst);
    for($j=0,$mx=0.;$j<=$np;$j++)
    { $difv[$j]=$soln->element($j+1,1); 
      if(abs($difv[$j])>$mx) { $mx=abs($difv[$j]); }
      $b[$j]-=$difv[$j];
    }
    #print $iter, " ", $b[0]," ",$b[1]," ",$b[2],"\n";
    while($mx>5.)
    { for($j=0;$j<=$np;$j++)
      { $difv[$j]/=2.; $b[$j]+=$difv[$j]; }
      $mx/=2.;
    }
    $iter++;
  }
  #  get inverse matrix
  $Ainv=$AA->inverse; $Ainv=-$Ainv;
  # negative to get covariance matrix
  for($j=1,$llk=$b[0]*$ysum;$j<=$np;$j++) { $llk+=$b[$j]*$sumv[$j]; }
  for($i=1;$i<=$n;$i++)
  { for($j=1,$tem=$b[0];$j<=$np;$j++) { $tem+=$b[$j]*($xmat[$j-1][$i-1]); }
    $q=1./(1.+exp($tem)); $p=1.-$q;
    $llk+=log($q); 
  }
  $aic = -2.*($llk-$np1);
  $bic = -2.*($llk-log($n)*$np1);
  $llr = -2*($l0-$llk);
  #print "cov matrix of betas\n";
  #print $Ainv,"\n";
  #print "null llk=  ", $l0,"\n";
  #print "model llk= ", $llk,"\n";
  #print " estimate    SE       ratio\n";
  for($k=0;$k<=$np;$k++)
  { $seb[$k]=sqrt($Ainv->element($k+1, $k+1));
    #printf("%9.4f %9.4f %9.4f\n", $b[$k],$seb[$k], $b[$k]/$seb[$k]);
  }
  #print "aic = ", $aic,"\n";
  #print "bic = ", $bic,"\n";
  #print "loglikratio = ", $llr,"\n";
  # return some stuff
  @out=($n,$np,@b,@seb,$l0,$llk,$aic,$bic,$llr,@A);
  @out;
}

# input is output from logreg
#output:  @outp=($nh,@pred); vector of predicted probabilities from holdout
sub logregpred
{ local ($nh,$i,$j,$nn,$np,$np1);
  local ($tem,$p,$q,$xmat,$xvec,$b);
  my $nh = shift @_;
  my $np = shift @_;
  $np1=$np+1;
  $nn=$#_;
  @xvec=@_[0..($nn-$np1)];
  @b=@_[($nn-$np)..$nn];
  for($j=1;$j<=$np;$j++)
  { $i1=($j-1)*$nh; $i2=$j*$nh-1;
    if($j==1) { @xmat=[@xvec[$i1..$i2]]; }
    else { @xmat=(@xmat,[@xvec[$i1..$i2]]); }
  }
  @pred=(1..$nh);
  for($i=1;$i<=$nh;$i++)
  { for($j=1,$tem=$b[0];$j<=$np;$j++) { $tem+=$b[$j]*($xmat[$j-1][$i-1]); }
    $q=1./(1.+exp($tem)); $p=1.-$q;
    $pred[$i-1]=$p;
  }
  @outp=($nh,@pred);
  @outp;
}

# input nh=holdset number of cases
# y = observed binary vector
# pred=vector of predicted probabilities 
# cutoff = cutoff point for classify as 1 (need not be 0.5)
sub logregclass
{ local ($nh,$i,$nn,$yy,$ypred,$cl,$tb);
  my $nh = shift @_;
  $nn=$#_;
  $cut=@_[$nn];
  @yy=@_[0..($nh-1)];
  @ypred=@_[$nh..(2*$nh-1)];
  @cl=(0..($nh-1));
  for($i=0;$i<$nh;$i++)
  { if($ypred[$i]>=$cut) { $cl[$i]=1; } else { $cl[$i]=0; } }
  # classify into 2-way table 00=0 01=1 10=2 11=3
  @tb=(0..3); for($i=0;$i<=3;$i++) { $tb[$i]=0; }
  for($i=0;$i<$nh;$i++)
  { if($yy[$i]==0 && $cl[$i]==0) { $tb[0]++; }
    elsif($yy[$i]==0 && $cl[$i]==1) { $tb[1]++; }
    elsif($yy[$i]==1 && $cl[$i]==0) { $tb[2]++; }
    else { $tb[3]++; }
  }
  @tb; # 4-vector
}


# data sets Pima.tr and Pima.te
@npreg=(5,7,5,0,0,5,3,1,3,2,0,9,1,12,1,4,1,11,1,0,2,1,4,0,1,9,1,0,5,2,1,3,0,3,2,13,1,1,0,0,8,1,4,1,0,2,5,2,3,1,3,2,8,1,1,4,1,1,2,8,7,1,10,6,6,5,6,3,0,0,5,3,8,2,1,1,2,2,3,12,2,4,6,10,2,2,1,7,2,1,1,4,7,1,1,12,4,2,2,10,3,4,3,3,4,6,6,0,1,1,4,4,7,8,1,0,9,4,1,9,1,1,12,1,5,2,2,1,1,7,3,7,6,2,3,0,4,0,1,0,1,0,2,1,3,6,2,0,3,8,12,9,0,0,5,7,2,2,8,3,3,2,1,4,2,4,0,6,3,1,6,5,7,4,0,7,1,7,9,2,2,12,0,8,9,0,14,14,0,1,0,5,1,2,1,2,7,0,1,8);

@glu=(86,195,77,165,107,97,83,193,142,128,137,154,189,92,86,99,109,143,149,139,99,100,83,101,87,164,99,140,108,110,79,148,121,158,105,145,79,71,102,119,176,97,129,97,86,125,123,92,171,199,116,83,154,114,106,127,124,109,123,167,184,96,129,92,109,139,134,106,131,135,158,112,181,121,168,144,101,96,107,121,100,154,125,125,122,114,115,114,115,130,79,112,150,91,100,140,110,94,84,148,61,117,99,80,154,103,111,124,143,81,189,116,103,124,71,137,112,148,136,145,93,107,151,97,144,112,99,109,120,187,129,179,80,105,191,95,99,137,97,100,167,180,122,90,120,154,56,177,124,85,88,152,198,188,139,168,197,142,126,158,130,100,164,95,122,85,151,144,111,107,115,105,194,184,95,124,111,137,57,157,95,140,117,100,123,138,100,175,74,133,119,155,128,112,140,141,129,106,118,155);

@bp=(68,70,82,76,60,76,58,50,80,78,40,78,60,62,66,76,60,94,68,62,70,66,86,64,68,84,58,65,72,74,60,66,66,64,80,82,80,48,86,66,90,68,60,64,68,60,74,76,72,76,74,66,78,66,70,88,74,38,48,106,84,64,76,62,60,80,70,54,66,94,84,74,68,70,88,82,58,68,62,78,64,72,78,70,76,68,70,76,64,60,75,78,78,54,72,82,76,76,50,84,82,62,80,82,62,72,64,70,74,74,110,72,66,76,78,84,82,60,74,80,56,72,70,70,82,86,52,56,80,68,92,95,66,58,68,80,72,68,70,88,74,90,70,62,70,78,56,60,80,55,74,78,66,82,64,88,70,82,74,76,78,54,82,60,52,58,90,72,90,68,60,72,68,78,85,70,62,90,80,74,54,85,66,74,70,60,78,62,52,102,64,84,48,68,74,58,68,70,58,62);

@skin=(28,33,41,43,25,27,31,16,15,37,35,30,23,7,52,15,8,33,29,17,16,29,19,17,34,21,10,26,43,29,42,25,30,13,45,19,25,18,17,27,34,21,12,19,32,20,40,20,33,43,15,23,32,36,28,11,36,18,32,46,33,27,28,32,27,35,23,21,40,46,41,30,36,32,29,46,17,13,13,17,23,29,31,26,27,22,30,17,22,23,30,40,29,25,12,43,20,18,23,48,28,12,11,31,31,32,39,20,22,41,31,12,32,24,50,27,32,27,50,46,11,30,40,40,26,42,15,21,48,39,49,31,30,40,15,45,17,14,15,60,17,26,27,12,30,41,28,29,33,20,40,34,32,14,35,42,99,18,38,36,23,28,43,32,43,22,46,27,12,19,39,29,28,39,25,33,13,41,37,35,14,33,31,40,44,35,25,30,10,28,18,44,45,22,26,34,49,37,36,26);

@bmi=(30.2,25.1,35.8,47.9,26.4,35.6,34.3,25.9,32.4,43.3,43.1,30.9,30.1,27.6,41.3,23.2,25.4,36.6,29.3,22.1,20.4,32.0,29.3,21.0,37.6,30.8,25.4,42.6,36.1,32.4,43.5,32.5,34.3,31.2,33.7,22.2,25.4,20.4,29.3,38.8,33.7,27.2,27.5,18.2,35.8,33.8,34.1,24.2,33.3,42.9,26.3,32.2,32.4,38.1,34.2,34.5,27.8,23.1,42.1,37.6,35.5,33.2,35.9,32.0,25.0,31.6,35.4,30.9,34.3,40.6,39.4,31.6,30.1,39.1,35.0,46.1,24.2,21.1,22.9,26.5,29.7,31.3,27.6,31.1,35.9,28.7,34.6,23.8,30.8,28.6,32.0,39.4,35.2,25.2,25.3,39.2,28.4,31.6,30.4,37.6,34.4,29.7,19.3,34.2,32.8,37.7,34.2,27.4,26.2,46.3,28.5,22.1,39.1,28.7,33.2,27.3,34.2,30.9,37.4,37.9,22.5,30.8,41.8,38.1,32.0,38.4,24.6,25.2,38.9,37.7,36.4,34.2,26.2,34.9,30.9,36.5,25.6,24.8,18.2,46.8,23.4,36.5,36.8,27.2,42.9,46.1,24.2,34.6,33.2,24.4,35.3,34.2,41.3,32.0,28.6,38.2,34.7,24.7,25.9,31.6,28.4,37.8,32.8,35.4,36.2,27.8,42.1,33.9,28.4,26.5,33.7,36.9,35.9,37.0,37.4,25.5,24.0,32.0,32.8,39.4,26.1,37.4,30.8,39.4,33.1,34.6,36.6,33.6,27.8,32.8,34.9,38.7,40.5,34.1,24.1,25.4,38.5,39.4,33.3,34.0);

@ped=(0.364,0.163,0.156,0.259,0.133,0.378,0.336,0.655,0.200,1.224,2.288,0.164,0.398,0.926,0.917,0.223,0.947,0.254,0.349,0.207,0.235,0.444,0.317,0.252,0.401,0.831,0.551,0.431,0.263,0.698,0.678,0.256,0.203,0.295,0.711,0.245,0.583,0.323,0.695,0.259,0.467,1.095,0.527,0.299,0.238,0.088,0.269,1.698,0.199,1.394,0.107,0.497,0.443,0.289,0.142,0.598,0.100,0.407,0.520,0.165,0.355,0.289,0.280,0.085,0.206,0.361,0.542,0.292,0.196,0.284,0.395,0.197,0.615,0.886,0.905,0.335,0.614,0.647,0.678,0.259,0.368,0.338,0.565,0.205,0.483,0.092,0.529,0.466,0.421,0.692,0.396,0.236,0.692,0.234,0.658,0.528,0.118,0.649,0.968,1.001,0.243,0.380,0.284,1.292,0.237,0.324,0.260,0.254,0.256,1.096,0.680,0.463,0.344,0.687,0.422,0.231,0.260,0.150,0.399,0.637,0.417,0.821,0.742,0.218,0.452,0.246,0.637,0.833,1.162,0.254,0.968,0.164,0.313,0.225,0.299,0.330,0.294,0.143,0.147,0.962,0.447,0.314,0.340,0.580,0.452,0.571,0.332,1.072,0.305,0.136,0.378,0.893,0.502,0.682,0.411,0.787,0.575,0.761,0.162,0.851,0.323,0.498,0.341,0.284,0.816,0.306,0.371,0.255,0.495,0.165,0.245,0.159,0.745,0.264,0.247,0.161,0.138,0.391,0.096,0.134,0.748,0.244,0.493,0.661,0.374,0.534,0.412,0.212,0.269,0.234,0.725,0.619,0.613,0.315,0.828,0.699,0.439,0.605,0.261,0.543);

@age=(24,55,35,26,23,52,25,24,63,31,33,45,59,44,29,21,21,51,42,21,27,42,34,21,24,32,21,24,33,27,23,22,33,24,29,57,22,22,27,22,58,22,31,21,25,31,28,28,24,22,24,22,45,21,22,28,30,26,26,43,41,21,39,46,27,25,29,24,22,26,29,25,60,23,52,46,23,26,23,62,21,37,49,41,26,25,32,31,21,21,22,38,54,23,28,58,27,23,21,51,46,30,30,27,23,55,24,36,21,32,37,37,31,52,21,59,36,29,24,40,22,24,38,30,58,28,21,23,41,41,32,60,41,25,34,26,28,21,21,31,33,35,27,24,30,27,22,21,26,42,48,33,28,22,26,40,62,21,39,28,34,24,50,28,28,28,21,40,29,24,40,28,41,31,24,37,23,39,41,30,22,41,22,43,40,21,46,38,22,45,23,34,24,26,23,24,43,22,23,46);

@type=(0,1,0,0,0,1,0,0,0,1,1,0,1,1,0,0,0,1,1,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,1,0,1,1,1,0,1,1,0,0,1,0,0,0,1,1,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,1,0,1,0,1,0,0,0,1,0,0,0,0,1,1,0,0,1,1,0,1,0,0,1,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,1,1,1,0,1,1,0,0,1,1,0,0,0,0,0,1,0,0,0,1,0,1,1,1,0,0,0,0,0,0,0,0,1,0,1,1,1,0,1,0,0,1,0,0,0,1,0,0,1);

# holdout
@npreg1=(6,1,1,3,2,5,0,1,3,9,1,5,3,10,4,9,2,4,3,7,9,0,2,1,1,5,7,1,0,5,0,1,4,2,4,5,2,3,7,0,13,2,15,4,7,2,6,1,1,1,4,1,7,4,0,2,1,1,4,3,8,0,0,5,2,2,2,9,1,7,2,17,4,7,0,6,2,8,0,1,8,4,0,0,5,8,2,0,5,6,5,7,1,1,3,4,0,6,3,7,0,1,4,6,2,9,0,9,1,2,1,3,11,4,5,2,10,2,2,7,0,2,7,7,1,4,0,0,6,2,0,2,2,10,0,2,2,3,7,3,13,2,1,1,2,6,8,2,1,1,1,8,1,3,1,1,5,5,4,5,3,3,0,0,1,0,0,1,1,1,5,8,3,1,2,3,3,5,9,1,6,1,1,3,4,3,1,2,0,8,1,1,3,1,1,0,4,0,0,0,1,0,9,9,8,5,0,0,1,0,4,3,0,0,0,1,0,2,2,4,4,2,6,2,3,7,3,12,3,3,9,6,2,3,2,1,0,3,10,4,1,8,5,4,1,3,1,1,11,6,1,3,2,1,1,6,3,11,2,2,6,1,1,1,1,1,0,3,6,11,2,9,0,4,4,0,1,7,1,3,0,11,1,2,2,1,4,10,6,6,10,3,0,0,2,1,7,3,6,2,2,5,10,0,7,3,10,1,5,1,0,2,2,0,8,2,11,3,1,13,12,1,3,1,3,0,1,2,9,10,5,1);

@glu1=(148,85,89,78,197,166,118,103,126,119,97,109,88,122,103,102,90,111,180,106,171,180,71,103,101,88,150,73,105,99,109,95,146,100,129,95,112,113,83,101,106,100,136,123,81,92,93,122,81,126,144,89,160,97,162,107,88,117,173,170,84,100,93,106,108,106,90,156,153,152,88,163,151,102,114,104,75,179,129,128,109,109,113,108,111,196,81,147,109,125,85,142,100,87,162,197,117,134,74,181,179,91,91,119,146,184,165,124,111,90,113,111,155,95,96,128,101,108,100,106,104,108,133,136,119,96,78,107,151,146,126,144,120,161,128,124,155,113,109,115,152,112,157,122,102,105,118,87,95,130,95,126,139,99,125,196,189,103,147,99,81,173,84,98,87,93,105,90,125,119,116,105,100,131,127,96,193,136,72,172,102,112,143,173,144,129,119,94,102,151,181,95,89,80,90,189,117,180,104,120,82,91,134,120,74,88,124,97,144,137,132,158,123,84,135,139,173,83,89,99,125,81,154,117,84,94,96,84,99,163,145,129,68,87,122,77,127,128,90,84,88,186,187,131,116,84,88,84,103,99,99,111,98,143,119,108,176,111,112,82,123,89,108,124,181,92,152,174,105,138,68,112,94,90,94,102,128,97,100,103,179,136,117,155,101,112,145,111,98,165,68,123,162,95,129,107,142,169,80,127,93,126,129,134,187,173,94,108,117,116,141,174,106,126,65,99,120,102,109,153,100,147,187,121,108,181,128,88,170,101,121,93);

@bp1=(72,66,66,50,70,72,84,30,88,80,66,75,58,78,60,76,68,72,64,92,110,66,70,80,50,66,66,50,64,74,88,66,85,66,86,72,66,44,78,65,72,68,70,80,78,62,50,90,72,56,58,76,54,60,76,74,30,88,70,64,74,70,60,82,52,64,70,86,82,88,74,72,90,74,80,74,64,72,110,98,76,64,80,68,72,76,60,85,62,68,74,60,66,78,52,70,80,80,68,84,90,64,70,50,76,85,90,70,86,80,64,56,76,70,74,64,86,62,70,60,64,62,88,74,86,56,88,62,62,70,84,58,76,68,68,68,74,50,80,66,90,75,72,64,86,70,72,58,60,70,74,88,46,62,50,76,64,108,74,54,86,82,64,82,60,100,68,62,70,54,74,100,68,64,58,56,70,84,78,68,90,72,84,84,82,64,88,68,64,78,64,82,74,74,68,104,64,78,64,74,64,68,74,72,70,78,56,64,82,70,86,70,88,82,68,62,78,65,90,68,70,72,74,90,72,64,78,72,54,70,88,90,70,60,60,56,80,72,85,90,78,90,76,68,70,68,62,64,68,60,72,58,60,86,44,44,86,84,78,52,72,24,88,60,78,62,82,58,80,74,62,82,70,88,65,78,82,76,74,72,50,84,60,52,58,80,82,70,58,68,106,100,76,64,74,50,90,74,80,46,64,78,62,58,50,78,72,60,86,78,84,88,56,86,72,60,80,44,58,88,84,94,70,78,62,88,88,58,74,76,72,70);

@skin1=(35,29,23,32,45,19,47,38,41,35,15,26,11,31,33,37,42,47,25,18,24,39,27,11,15,21,42,10,41,27,30,13,27,20,20,33,22,13,26,28,54,25,32,15,40,28,30,51,18,29,28,34,32,23,56,30,42,24,14,37,31,26,25,30,26,35,17,28,42,44,19,41,38,40,34,18,24,42,46,41,39,44,16,20,28,29,22,54,41,30,22,33,15,27,38,39,31,37,28,21,27,24,32,22,35,15,33,33,19,14,35,39,28,32,18,42,37,32,52,24,23,10,15,26,39,17,29,30,31,38,29,33,37,23,19,28,17,10,31,39,33,32,21,32,36,32,19,16,18,13,21,36,19,19,40,36,33,37,25,28,16,48,22,15,37,39,22,18,24,13,29,36,23,14,24,34,31,41,25,49,39,30,23,33,32,29,41,18,46,32,30,25,16,11,8,25,27,63,37,18,13,32,33,22,40,30,13,36,40,38,31,30,37,31,42,41,32,28,30,38,18,15,32,19,32,25,39,31,19,18,34,7,32,18,18,30,37,25,32,23,29,35,27,21,28,30,24,23,40,19,30,31,17,30,47,20,27,40,50,22,45,19,19,32,42,25,39,22,28,26,13,24,27,47,22,40,17,32,12,30,36,35,23,27,35,45,18,27,33,26,23,35,36,39,26,19,24,19,36,21,32,27,36,20,33,39,18,46,30,29,26,37,27,27,23,17,37,20,18,37,33,41,22,39,24,44,39,26,31,48,23,31);

@bmi1=(33.6,26.6,28.1,31.0,30.5,25.8,45.8,43.3,39.3,29.0,23.2,36.0,24.8,27.6,24.0,32.9,38.2,37.1,34.0,22.7,45.4,42.0,28.0,19.4,24.2,24.4,34.7,23.0,41.5,29.0,32.5,19.6,28.9,32.9,35.1,37.7,25.0,22.4,29.3,24.6,36.6,38.5,37.1,32.0,46.7,31.6,28.7,49.7,26.6,28.7,29.5,31.2,30.5,28.2,53.2,33.6,55.0,34.5,29.7,34.5,38.3,30.8,28.7,39.5,32.5,30.5,27.3,34.3,40.6,50.0,29.0,40.9,29.7,37.2,44.2,29.9,29.7,32.7,67.1,32.0,27.9,34.8,31.0,27.3,23.9,37.5,27.7,42.8,35.8,30.0,29.0,28.8,23.6,34.6,37.2,36.7,45.2,46.2,29.7,35.9,44.1,29.2,33.1,27.1,38.2,30.0,52.3,35.4,30.1,24.4,33.6,30.1,33.3,32.1,33.6,40.0,45.6,25.2,40.5,26.5,27.8,25.3,32.4,26.0,45.6,20.8,36.9,36.6,35.5,28.0,30.7,31.6,39.7,25.5,30.5,32.9,26.6,29.5,35.9,38.1,26.8,35.7,25.6,35.1,45.5,30.8,23.1,32.7,23.9,25.9,25.9,38.5,28.7,21.8,33.3,36.5,31.2,39.2,34.9,34.0,27.5,38.4,35.8,25.2,37.2,43.4,20.0,25.1,24.3,22.3,32.3,43.3,31.6,23.7,27.7,24.7,34.9,35.0,31.6,42.4,35.7,34.4,42.4,35.7,38.5,26.4,45.3,26.0,40.6,42.9,34.1,35.0,30.4,30.0,24.5,34.3,33.2,59.4,33.6,30.5,21.2,39.9,25.9,20.8,35.3,27.6,21.8,36.8,41.3,33.2,28.0,35.5,35.2,38.2,42.3,40.7,46.5,36.8,33.5,32.8,28.9,30.1,29.3,25.2,37.2,33.3,37.3,29.7,25.6,31.6,30.3,19.6,25.0,21.8,29.8,33.3,36.3,32.4,34.9,39.5,32.0,34.5,43.6,33.1,27.4,31.9,29.9,36.9,46.2,26.9,38.6,29.5,34.7,30.1,35.5,24.0,33.3,46.8,39.4,28.5,33.6,27.8,27.1,35.8,40.0,19.5,41.5,32.9,32.5,36.1,20.1,28.2,43.5,37.7,24.7,34.5,27.5,40.9,19.5,27.6,37.8,28.3,33.8,38.7,21.8,34.8,32.5,27.5,34.0,33.6,35.5,57.3,49.6,44.6,33.2,28.3,30.4,29.9,39.8,34.4,38.0,29.6,41.2,26.4,33.9,33.8,23.1,35.5,39.1,36.1,32.4,44.5,29.0,27.4,32.0,36.6,42.3,30.8,28.5,40.6,30.0,49.3,36.4,39.0,26.0,43.3,36.5,28.4,44.0,32.9,26.2,30.4);

@ped1=(0.627,0.351,0.167,0.248,0.158,0.587,0.551,0.183,0.704,0.263,0.487,0.546,0.267,0.512,0.966,0.665,0.503,1.390,0.271,0.235,0.721,1.893,0.586,0.491,0.526,0.342,0.718,0.248,0.173,0.203,0.855,0.334,0.189,0.867,0.231,0.370,0.307,0.140,0.767,0.237,0.178,0.324,0.153,0.443,0.261,0.130,0.356,0.325,0.283,0.801,0.287,0.192,0.588,0.443,0.759,0.404,0.496,0.403,0.361,0.356,0.457,0.597,0.532,0.286,0.318,1.400,0.085,1.189,0.687,0.337,0.229,0.817,0.294,0.204,0.167,0.722,0.370,0.719,0.319,1.321,0.640,0.905,0.874,0.787,0.407,0.605,0.290,0.375,0.514,0.464,1.224,0.687,0.666,0.101,0.652,2.329,0.089,0.238,0.293,0.586,0.686,0.192,0.446,1.318,0.329,1.213,0.427,0.282,0.143,0.249,0.543,0.557,1.353,0.612,0.997,1.101,1.136,0.128,0.677,0.296,0.454,0.881,0.262,0.647,0.808,0.340,0.434,0.757,0.692,0.337,0.520,0.422,0.215,0.326,1.391,0.875,0.433,0.626,1.127,0.150,0.731,0.148,0.123,0.692,0.127,0.122,1.476,0.166,0.260,0.472,0.673,0.349,0.654,0.279,0.962,0.875,0.583,0.305,0.385,0.499,0.306,2.137,0.545,0.299,0.509,1.021,0.236,1.268,0.221,0.205,0.660,0.239,0.949,0.389,1.600,0.944,0.241,0.286,0.280,0.702,0.674,0.528,1.076,0.258,0.554,0.219,0.507,0.561,0.496,0.516,0.328,0.233,0.551,0.527,1.138,0.435,0.230,2.420,0.510,0.285,0.415,0.381,0.460,0.733,0.705,0.258,0.452,0.600,0.607,0.170,0.419,0.344,0.197,0.233,0.365,0.536,1.159,0.629,0.292,0.145,1.144,0.547,0.839,0.313,0.267,0.738,0.238,0.297,0.154,0.268,0.771,0.582,0.187,0.444,0.717,1.251,0.804,0.549,0.825,0.159,0.365,0.423,1.034,0.160,0.204,0.591,0.422,0.471,0.126,0.497,0.412,0.430,0.198,0.892,0.280,0.813,1.154,0.925,0.175,1.699,0.733,0.559,0.400,0.514,1.258,0.482,0.270,0.593,0.878,0.557,0.257,1.282,0.347,0.362,0.148,0.238,0.115,0.871,0.149,0.730,0.455,0.260,0.466,0.240,0.155,0.217,0.235,0.141,0.430,0.631,0.285,0.880,0.364,0.366,0.591,0.181,0.128,0.268,0.177,0.176,0.674,0.439,0.441,0.352,0.826,0.970,0.595,0.415,0.251,0.496,0.433,0.646,0.426,0.515,0.600,0.453,0.785,0.400,0.219,1.174,0.488,0.358,0.408,0.261,0.223,0.222,1.057,0.766,0.403,0.171,0.245,0.315);

@age1=(50,31,21,26,53,51,31,33,27,29,22,60,22,45,33,46,27,56,26,48,54,25,22,22,26,30,42,21,22,32,38,25,27,28,23,27,24,22,36,22,45,26,43,34,42,24,23,31,24,21,37,23,39,22,25,23,26,40,33,30,39,21,22,38,22,34,22,42,23,36,22,47,36,45,27,41,33,36,26,33,31,26,21,32,27,57,25,24,25,32,32,61,26,22,24,31,24,46,23,51,23,21,22,33,29,49,23,34,23,24,21,30,51,24,43,24,38,21,25,29,23,22,37,51,29,26,21,25,28,29,24,25,29,47,25,30,27,25,43,28,43,21,24,30,23,37,46,25,22,22,36,49,22,26,28,29,29,65,30,30,22,25,21,22,22,35,22,25,25,24,35,45,28,21,25,39,25,35,38,28,28,25,22,22,37,28,26,21,21,36,38,43,38,22,36,41,24,25,22,26,23,25,81,48,39,37,21,25,28,22,63,35,29,23,24,21,58,24,42,33,45,25,39,21,28,41,40,46,24,28,53,60,25,21,22,24,23,27,56,25,29,37,53,28,21,25,23,28,42,32,21,22,22,23,25,35,52,45,24,25,34,21,24,21,22,25,27,36,26,50,23,50,21,29,21,24,22,32,28,27,22,42,27,25,22,24,70,40,43,49,47,22,26,22,25,29,43,31,28,22,23,40,38,21,34,31,56,24,42,25,22,24,22,21,42,21,48,26,22,39,46,27,36,28,25,26,37,22,43,63,30,23);

@type1=(1,0,0,1,1,1,1,0,0,1,0,0,0,0,0,1,1,1,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,1,0,1,1,1,1,0,0,0,0,0,0,0,1,0,1,0,1,0,0,0,1,0,1,1,1,1,1,0,0,0,1,0,0,1,0,1,0,0,0,1,0,0,1,0,1,1,0,0,1,0,1,0,0,0,0,1,0,1,0,0,0,1,0,0,1,0,0,0,0,1,0,0,1,0,1,0,1,0,1,1,1,1,0,1,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,1,1,1,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,1,0,1,0,1,0,0,0,0,0,1,0,0,1,1,0,1,0,0,1,1,0,0,0,0,1,0,0,0,1,1,0,0,1,0,1,0,1,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,1,0,0,1,1,0,1,0,0,0);

# perl code here
$ntrain=170;
#$ntrain=200;
$nhold=200;
#$nhold=332;
#$zcv=1.959964;
@itrain=isamp(200,$ntrain);
@ihold=isamp(332,$nhold); 

@x1=@npreg[@itrain];
@x2=@glu[@itrain];
@x3=@bp[@itrain];
@x4=@skin[@itrain];
@x5=@bmi[@itrain];
@x6=@ped[@itrain];
@x7=@age[@itrain];
@y=@type[@itrain];
@bfull=(-9.8,0.1,0.03,0.0,0.0,0.1,1.8,0.04);
@bsub=(-9.8,0.03,0.1,1.8,0.06);

$n=$ntrain; 
$nc1=7;
@out1=logreg($n,$nc1,@y,@x1,@x2,@x3,@x4,@x5,@x6,@x7,@bfull);

#  @out=($n,$np,@b,@seb,$l0,$llk,$aic,$bic,$llr,@A);
$n=$out1[0];
$nc=$out1[1];
@b1=@out1[2..(2+$nc1)]; # $nc+1 coefs
@seb1=@out1[(3+$nc1)..(3+2*$nc1)];
$nn1=3+2*$nc1;
$l01=$out1[$nn1+1];
$llk1=$out1[$nn1+2];
$aic1=$out1[$nn1+3];
$bic1=$out1[$nn1+4];
$llr1=$out1[$nn1+5];

$n=$ntrain; 
$nc2=4;
@out2=logreg($n,$nc2,@y,@x2,@x5,@x6,@x7,@bsub);
@b2=@out2[2..(2+$nc2)]; # $nc+1 coefs
@seb2=@out2[(3+$nc2)..(3+2*$nc2)];
$nn2=3+2*$nc2;
$l02=$out2[$nn2+1];
$llk2=$out2[$nn2+2];
$aic2=$out2[$nn2+3];
$bic2=$out2[$nn2+4];
$llr2=$out2[$nn2+5];

@x1h=@npreg1[@ihold];
@x2h=@glu1[@ihold];
@x3h=@bp1[@ihold];
@x4h=@skin1[@ihold];
@x5h=@bmi1[@ihold];
@x6h=@ped1[@ihold];
@x7h=@age1[@ihold];
@yh=@type1[@ihold];
$nh=$nhold;
# prediction
@out1pred=logregpred($nh,$nc1,@x1h,@x2h,@x3h,@x4h,@x5h,@x6h,@x7h,@b1);
$len1=$out1pred[0];
@pred1=@out1pred[1..$len1];
@out2pred=logregpred($nh,$nc2,@x2h,@x5h,@x6h,@x7h,@b2);
$len2=$out2pred[0];
@pred2=@out2pred[1..$len2];

# classify
$cut=0.5;
@tb1=logregclass($nh,@yh,@pred1,$cut);
@tb2=logregclass($nh,@yh,@pred2,$cut);

$cut2=0.3;
@tb1s=logregclass($nh,@yh,@pred1,$cut2);
@tb2s=logregclass($nh,@yh,@pred2,$cut2);


# add 1 to each element for R
for($i=0;$i<$ntrain;$i++) { $itrain[$i]=$itrain[$i]+1; }
for($i=0;$i<$nhold;$i++) { $ihold[$i]=$ihold[$i]+1; }
$n1=$ntrain;
$i1str="itrain=c(";
for($i=0;$i<$n1-1;$i++) { $i1str=$i1str . $itrain[$i] . ", " ; }
$i1str=$i1str . $itrain[$n1-1] . ")";
$n2=$nhold;
$i2str="ihold=c(";
for($i=0;$i<$n2-1;$i++) { $i2str=$i2str . $ihold[$i] . ", " ; }
$i2str=$i2str . $ihold[$n2-1] . ")";

# answers
$ansa=$b1[6];
$ansb=$b2[4];
$ansc1=$pred1[0];
$ansc2=$pred2[0];
$ansd1=$tb1[1]+$tb1[2];
$ansd2=$tb2[1]+$tb2[2];
if($ansd2<=$ansd1) { $anse=2; } else { $anse=1; }
$ansf1=$tb1s[1]+$tb1s[2];
$ansf2=$tb2s[1]+$tb2s[2];

#$BR
#full model
#$BR
#n is $ntrain. number of covariates is $nc or $nc1;
#$BR
#betahat is ($b1[0], $b1[1], $b1[2], $b1[3], $b1[4], $b1[5], $b1[6], $b1[7])
#$BR
#SEs are $seb1[0], $seb1[1], $seb1[2], $seb1[3], $seb1[4], $seb1[5], $seb1[6], $seb1[7];
#$BR
#aic is  $aic1; bic is $bic1; log-likelihood ratio is $llr1.
#$BR

#$BR
#sub model
#$BR
#n is $n. number of covariates is $nc2;
#$BR
#betahat is ($b2[0], $b2[1], $b2[2], $b2[3], $b2[4])
#$BR
#SEs are $seb2[0], $seb2[1], $seb2[2], $seb2[3], $seb2[4];
#$BR
#aic is  $aic2; bic is $bic2; log-likelihood ratio is $llr2.
#$BR
#$BR
#additions
#$BR
#$pred1[0],$pred1[1],$pred1[2],$pred1[3]
#$BR
#$tb1[0],$tb1[1],$tb1[2],$tb1[3]
#$BR
#$pred2[0],$pred2[1],$pred2[2],$pred2[3]
#$BR
#$tb2[0],$tb2[1],$tb2[2],$tb2[3]
#$BR
#$BR

Context()->texStrings;

BEGIN_TEXT

This question involves logistic regression analysis of the Pima data set in R
on risk factors for diabetes among Pima women. Your training and holding data sets
will be subsets of the Pima.tr and Pima.te data sets in  the library MASS.
The binary response variable is "type" (type=Yes for Diabetes,
type=No for no diabetes).

$BR
Get your training set and holdout set with the following in R.
$BR
$BR
$i1str
$BR
$BR
$i2str
$BR
$BR
library(MASS)
$BR
data(Pima.tr)
$BR
mytrain=Pima.tr[itrain,]
$BR
data(Pima.te)
$BR
myhold=Pima.te[ihold,]
$BR
$BR

Next do the following:
$BR
(1) Fit the logistic regression model with all 7 explanatory variables
npreg, glu, bp, skin, bmi, ped, age. Call this model 1.
$BR
(2) Fit the logistic regression model with 4 explanatory variables
glu, bmi, ped, age (this is best model from backward elimination
if all cases of Pima.tr is used).
For this model with 4 explanatory variables, call it model 2.
$BR
(3) Apply both models 1 and 2 to the holdout data set and get the
predicted probabilities. Classify a case as $BITALIC diabetes  $EITALIC
if the predicted probability exceeds (>=) 0.5 and otherwise
classify it as $BITALIC non-diabetes  $EITALIC.
$BR
(4) For models 1 and 2, get the total number of misclassifications.
Which model is better based on this criterion?
$BR
(5) For models 1 and 2, compare the misclassification tables
if one classifies a case as $BITALIC diabetes  $EITALIC
if the predicted probability exceeds (>=) 0.3 and otherwise
classify it as $BITALIC non-diabetes  $EITALIC.
Which is the better boundary to use?
$BR
$BR

You will be asked to supply some numbers below from doing the above.
$BR

$BR
$BBOLD Part a) $EBOLD
$BR
For model 1, the regression coefficient for $BITALIC ped $EITALIC is
$BR
\{ ans_rule(8) \}
$BR

$BR
$BBOLD Part b) $EBOLD
$BR
For model 2, the regression coefficient for $BITALIC age $EITALIC is
$BR
\{ ans_rule(8) \}
$BR

$BR
$BBOLD Part c) $EBOLD
$BR
For the first subject in the holdout set, the predicted probability is:
$BR
\{ ans_rule(8) \} for model 1,
$BR
\{ ans_rule(8) \} for model 2.
$BR

$BR
$BBOLD Part d) $EBOLD
$BR
Use a boundary of 0.5 in the predicted probabilities to decide on
diabetes (predicted probability greater than or equal to 0.5) or non-diabetes.
The total number of misclassifications of the 200 cases in the holdout set is:
$BR
\{ ans_rule(8) \} for model 1,
$BR
\{ ans_rule(8) \} for model 2.
$BR

$BR
$BBOLD Part e) $EBOLD
$BR
With a boundary of 0.5 in predicted probabilities,
the better model with a lower misclassification rate is model :
$BR
\{ ans_rule(8) \} (enter 1 or 2, and enter model 2 in case of a tie).
$BR

$BR
$BBOLD Part f) $EBOLD
$BR
Use a boundary of 0.3 in the predicted probabilities to decide on
diabetes (predicted probability greater than or equal to 0.3) or non-diabetes.
The total number of misclassifications of the 200 cases in the holdout set is:
$BR
\{ ans_rule(8) \} for model 1,
$BR
\{ ans_rule(8) \} for model 2.
$BR

$BR
There is no question on the better boundary to use, because that
depends on the relative seriousness of the two types of misclassification
errors.

$BR
END_TEXT

#########################################################

BEGIN_HINT
See Section 6.2 of the course pack and the R code examples on the
course web site.
END_HINT

#########################################################
$showPartialCorrectAnswers = 1;
#########################################################
ANS( num_cmp($ansa,tol=> 0.010, tolType=>"absolute") );
ANS( num_cmp($ansb,tol=> 0.001, tolType=>"absolute") );
ANS( num_cmp($ansc1,tol=> 0.005, tolType=>"absolute") );
ANS( num_cmp($ansc2,tol=> 0.005, tolType=>"absolute") );
ANS( num_cmp($ansd1,tol=> 1.5, tolType=>"absolute") );
ANS( num_cmp($ansd2,tol=> 1.5, tolType=>"absolute") );
#ANS( num_cmp($ansd1) );
#ANS( num_cmp($ansd2) );
ANS( num_cmp($anse) );
#ANS( num_cmp($ansf1) );
#ANS( num_cmp($ansf2) );
ANS( num_cmp($ansf1,tol=> 1.5, tolType=>"absolute") );
ANS( num_cmp($ansf2,tol=> 1.5, tolType=>"absolute") );

#########################################################
BEGIN_SOLUTION
$BR
library(MASS)
$BR
data(Pima.tr)
$BR
mytrain=Pima.tr[itrain,]
$BR
data(Pima.te)
$BR
myhold=Pima.te[ihold,] 
$BR
fit1=glm(type~npreg+glu+bp+skin+bmi+ped+age,family=binomial,data=mytrain)
$BR
fit2=glm(type~glu+bmi+ped+age,family=binomial,data=mytrain)
$BR
print(summary(fit1))
$BR
print(summary(fit2))
$BR
pred1=predict(fit1,type="response",new=myhold)
$BR
pred2=predict(fit2,type="response",new=myhold)
$BR
tem=cbind(pred1,pred2)
$BR
print(tem[1:4,])
$BR
tab1=table(myhold$DOLLAR type,(pred1>=0.5))
$BR
tab2=table(myhold$DOLLAR type,(pred2>=0.5))
$BR
tab1s=table(myhold$DOLLAR type,(pred1>=0.3))
$BR
tab2s=table(myhold$DOLLAR type,(pred2>=0.3))
$BR
print(tab1); print(tab2)
$BR
print(tab1s); print(tab2s)
$BR
Answers are based on the R output.
$BR
END_SOLUTION
#########################################################


ENDDOCUMENT();

