########################################################
## DESCRIPTION
## multiple regression and cross-validation
## ENDDESCRIPTION
#########################################################

#########################################################
## KEYWORDS('statistics', 'multiple regression','cross-validation')
#########################################################

#########################################################
## DBsubject('Statistics')
## Date('2015/02/28')
## Author('H Joe')
## Institution('UBC')
## TitleText1('')
## EditionText1('')
## AuthorText1('')
## Section1('')
## Problem1('')
#########################################################

#########################################################

## Initializations:
DOCUMENT();

loadMacros(
  "PGstandard.pl",
  "PGchoicemacros.pl",
  "parserRadioButtons.pl",
  "MathObjects.pl",
  "parserMultiAnswer.pl",
  "answerHints.pl",
  "regrfnsPG.pl",   # functions for regression and sampling
);

# richmond townhouse, n=59 in full data set

Context("Matrix");
#@x0=(1,1,1,1,1,1,1,1,1,1, 1,1,1,1,1,1,1,1,1,1, 1,1,1,1,1,1,1,1,1,1, 1,1,1,1,1,1,1,1,1,1, 1,1,1,1,1,1,1,1,1,1, 1,1,1,1,1,1,1,1,1    );
@x0=(1,1,1,1,1,1,1,1,1,1, 1,1,1,1,1,1,1,1,1,1, 1,1,1,1,1,1,1,1,1,1, 1,1,1,1,1,1,1,1,1,1, 1,1,1,1,1,1,1,1,1,1    );

@x0ho=(1,1,1,1,1,1,1,1,1);


@askpr=(
73.9000, 57.8000, 55.8000, 54.8000, 54.9800, 86.8000, 65.8000, 57.8000,
57.5000, 25.9000, 78.8000, 44.8000, 53.9000, 108.8000, 56.8800, 50.8000,
68.8000, 53.8000, 81.9000, 68.5000, 49.9000, 79.9900, 51.6800, 50.8000,
56.8000, 48.5000, 40.9000, 45.9900, 52.4000, 59.8000, 58.8000, 33.7000,
40.8000, 60.8000, 77.8000, 79.8000, 50.5000, 46.8000, 47.8000, 65.9900,
26.9900, 40.8000, 68.5000, 54.8000, 41.9900, 74.8000, 62.9000, 62.8888,
51.9900, 61.5000, 47.9000, 68.8000, 58.3900, 71.9900, 73.8000, 55.2000,
58.6800, 53.8000, 48.8000);

@ffarea=(
15.15, 12.01, 13.06, 11.26, 13.06, 15.08, 13.45, 13.84, 13.46, 6.10,
19.48, 9.40, 11.84, 23.98, 15.78, 12.27, 15.95, 10.95, 20.95, 13.59,
15.60, 22.00, 15.10, 16.60, 15.50, 14.80, 16.06, 16.01, 16.22, 17.63,
17.37, 12.00, 12.26, 13.20, 16.50, 15.25, 12.26, 16.20, 13.34, 22.78,
10.50, 14.00, 15.76, 15.46, 12.90, 17.48, 14.00, 15.77, 12.09, 14.50,
12.10, 16.90, 15.09, 15.05, 17.54, 15.30, 13.96, 12.22, 14.80);

@beds=(
4, 3, 3, 2, 3, 3, 3, 3, 3, 1, 3, 2, 2, 3, 4, 2, 3, 2, 1, 3, 3, 3, 3, 4,
3, 3, 2, 3, 3, 5, 3, 2, 3, 3, 4, 2, 3, 4, 3, 2, 2, 3, 4, 3, 3, 4, 3, 3,
3, 3, 3, 4, 4, 3, 4, 3, 3, 3, 3);

@baths=(
3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 1.5, 3.5, 2.5, 2.5, 3.5,
3.5, 2.5, 3.5, 2.5, 3.5, 3.5, 3.5, 4.5, 2.5, 3.5, 3.5, 3.5, 3.5, 3.5, 2.5,
2.5, 3.5, 2.5, 2.5, 3.5, 4.5, 3.5, 3.5, 4.5, 3.5, 3.5, 1.5, 2.5, 3.5, 3.5,
2.5, 4.5, 3.5, 3.5, 3.5, 3.5, 2.5, 4.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 2.5);

@basement=( 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0.5, 0,
0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);


@age=(
0, 0, 0, 0, 1, 1, 1, 10, 10, 11, 11, 14, 15, 16, 17, 17, 18, 18, 19, 2,
20, 20, 20, 23, 23, 24, 25, 25, 25, 26, 26, 28, 29, 3, 3, 3, 3, 30, 32,
35, 37, 38, 4, 41, 44, 5, 5, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 50);

@mfee=(
22.2, 14.2, 18.6, 24.8, 19.6, 48.8, 18.2, 16.0, 22.1, 17.1, 20.4, 23.3,
21.0, 36.9, 17.3, 25.2, 23.6, 24.7, 34.8, 17.0, 27.0, 26.7, 24.5, 19.9,
17.4, 16.1, 24.4, 33.7, 36.4, 32.0, 31.0, 25.9, 19.8, 18.9, 25.4, 35.0,
18.0, 16.0, 24.5, 57.4, 28.0, 23.0, 22.1, 31.0, 23.2, 29.7, 19.6, 35.7,
18.1, 18.7, 18.0, 19.4, 20.3, 22.3, 18.2, 16.9, 22.0, 18.5, 25.0);

# $Xt and @y are global
sub multreg 
{ local($Xmat,$yt,$ymat,$n,$k,$XtX,$rhs,$LR,$dim,$beta,$base,$XtXinv);
  local($b,$seb,$yhat,$res,$rest,$sse11,$sse,$mse,$residSD);
  local($s1,$s2,$ybar,$sstot,$R2,$R2adj,$i,$j,$ij,$out);
  $Xmat=$Xt->transpose;
  ($n,$k) = $Xmat->dim();
  $yt=Matrix([@y]); $ymat=$yt->transpose;
  $XtX=$Xt*$Xmat; $rhs=$Xt*$ymat;
  $LR=$XtX->decompose_LR;
  #$L=$LR->L; #$R=$LR->R; #$PL=$LR->PL; #$PR=$LR->PR;
  ($dim,$beta,$base)=$LR->solve_LR($rhs);
  #$b1=$beta->element(2,1);
  @b=NULL; @seb=NULL;
  for($j=0;$j<$k;$j++) { $b[$j]=$beta->element($j+1,1); }
  $XtXinv=$XtX->inverse;
  # yhat and residual vectors, residual SD, R2, adjusted R2
  $yhat=$Xmat*$beta;
  $res=$ymat-$yhat; $rest=$res->transpose;
  $sse11=$rest*$res; $sse=$sse11->element(1,1);
  $mse=$sse/($n-$k); $residSD=sqrt($mse);
  for($j=0;$j<$k;$j++) { $seb[$j]=$residSD*sqrt($XtXinv->element($j+1,$j+1)); }
  $s1=0; $s2=0;
  for($i=0;$i<$n;$i++)
  { $s1=$s1+$y[$i]; $s2=$s2+$y[$i]**2; }
  $ybar=$s1/$n;
  $sstot=$s2-$n*$ybar*$ybar;
  $R2=1-$sse/$sstot;
  $R2adj=1-$mse/($sstot/($n-1));
  @xtxivec=NULL; $ij=0;
  for($i=0;$i<$k;$i++) 
  { for($j=0;$j<$k;$j++) 
    { $xtxivec[$ij]=$XtXinv->element($i+1,$j+1); $ij++; }
  }
  @out=($n,$k,$ybar,$sse,$sstot,$residSD,$R2,$R2adj,@b,@seb,@xtxivec);
  @out;
}

# $Xt and @y are global
# output also with cvrmse
sub multregcvrmse 
{ local($Xmat,$yt,$ymat,$n,$k,$XtX,$rhs,$LR,$dim,$beta,$base,$XtXinv);
  local($b,$seb,$yhat,$res,$rest,$sse11,$sse,$mse,$residSD);
  local($s1,$s2,$ybar,$sstot,$R2,$R2adj,$i,$j,$ij,$out);
  local($xrow,$xcol,$temmat,$Pii,$cvrmse,$cvres);
  $Xmat=$Xt->transpose;
  ($n,$k) = $Xmat->dim();
  $yt=Matrix([@y]); $ymat=$yt->transpose;
  $XtX=$Xt*$Xmat; $rhs=$Xt*$ymat;
  $LR=$XtX->decompose_LR;
  #$L=$LR->L; #$R=$LR->R; #$PL=$LR->PL; #$PR=$LR->PR;
  ($dim,$beta,$base)=$LR->solve_LR($rhs);
  #$b1=$beta->element(2,1);
  @b=NULL; @seb=NULL;
  for($j=0;$j<$k;$j++) { $b[$j]=$beta->element($j+1,1); }
  $XtXinv=$XtX->inverse;
  # yhat and residual vectors, residual SD, R2, adjusted R2
  $yhat=$Xmat*$beta;
  $res=$ymat-$yhat; $rest=$res->transpose;
  $sse11=$rest*$res; $sse=$sse11->element(1,1);
  $mse=$sse/($n-$k); $residSD=sqrt($mse);
  for($j=0;$j<$k;$j++) { $seb[$j]=$residSD*sqrt($XtXinv->element($j+1,$j+1)); }
  $s1=0; $s2=0;
  for($i=0;$i<$n;$i++)
  { $s1=$s1+$y[$i]; $s2=$s2+$y[$i]**2; }
  $ybar=$s1/$n;
  $sstot=$s2-$n*$ybar*$ybar;
  $R2=1-$sse/$sstot;
  $R2adj=1-$mse/($sstot/($n-1));
  @xtxivec=NULL; $ij=0;
  for($i=0;$i<$k;$i++) 
  { for($j=0;$j<$k;$j++) 
    { $xtxivec[$ij]=$XtXinv->element($i+1,$j+1); $ij++; }
  }
  $cvrmse=0;
  @cvres=(1..$n);
  for($j=0;$j<$k;$j++) { $b[$j]=$beta->element($j+1,1); }
  for($i=0;$i<$n;$i++)
  { $xcol=$Xt->column($i+1);
    $xrow=$Xmat->row($i+1);
    $temmat=$xrow*$XtXinv*$xcol;
    $Pii=$temmat->element(1,1);
    #$cvres[$i]=($res->element($i+1,1));
    $cvres[$i]=($res->element($i+1,1))/(1-$Pii);
    $cvrmse=$cvrmse+$cvres[$i]**2;
  }
  $cvrmse=sqrt($cvrmse/$n);
  @out=($n,$k,$ybar,$sse,$sstot,$residSD,$R2,$R2adj,@b,@seb,@xtxivec,@cvres,$cvrmse);
  @out;
}

# $Xt and @y are global, also $Xtho, @yho
# output also with cvrmse and holdout prediction errors
sub multregcvrmse2 
{ local($Xmat,$yt,$ymat,$n,$k,$XtX,$rhs,$LR,$dim,$beta,$base,$XtXinv);
  local($b,$seb,$yhat,$res,$rest,$sse11,$sse,$mse,$residSD);
  local($s1,$s2,$ybar,$sstot,$R2,$R2adj,$i,$j,$ij,$out);
  local($nho,$kho,$Xmatho,$yhatho,$ytho,$ymatho,$cvres,$cvrest,$tem,$resho,$cvrmse);
  $Xmat=$Xt->transpose;
  ($n,$k) = $Xmat->dim();
  $yt=Matrix([@y]); $ymat=$yt->transpose;
  $XtX=$Xt*$Xmat; $rhs=$Xt*$ymat;
  $LR=$XtX->decompose_LR;
  #$L=$LR->L; #$R=$LR->R; #$PL=$LR->PL; #$PR=$LR->PR;
  ($dim,$beta,$base)=$LR->solve_LR($rhs);
  #$b1=$beta->element(2,1);
  @b=NULL; @seb=NULL;
  for($j=0;$j<$k;$j++) { $b[$j]=$beta->element($j+1,1); }
  $XtXinv=$XtX->inverse;
  # yhat and residual vectors, residual SD, R2, adjusted R2
  $yhat=$Xmat*$beta;
  $res=$ymat-$yhat; $rest=$res->transpose;
  $sse11=$rest*$res; $sse=$sse11->element(1,1);
  $mse=$sse/($n-$k); $residSD=sqrt($mse);
  for($j=0;$j<$k;$j++) { $seb[$j]=$residSD*sqrt($XtXinv->element($j+1,$j+1)); }
  $s1=0; $s2=0;
  for($i=0;$i<$n;$i++)
  { $s1=$s1+$y[$i]; $s2=$s2+$y[$i]**2; }
  $ybar=$s1/$n;
  $sstot=$s2-$n*$ybar*$ybar;
  $R2=1-$sse/$sstot;
  $R2adj=1-$mse/($sstot/($n-1));
  @xtxivec=NULL; $ij=0;
  for($i=0;$i<$k;$i++) 
  { for($j=0;$j<$k;$j++) 
    { $xtxivec[$ij]=$XtXinv->element($i+1,$j+1); $ij++; }
  }
  for($j=0;$j<$k;$j++) { $seb[$j]=$residSD*sqrt($XtXinv->element($j+1,$j+1)); }
  for($j=0;$j<$k;$j++) { $b[$j]=$beta->element($j+1,1); }
  # addition for holdout sample
  $Xmatho=$Xtho->transpose;
  ($nho,$kho) = $Xmatho->dim();
  $ytho=Matrix([@yho]); $ymatho=$ytho->transpose;
  $yhatho=$Xmatho*$beta;
  $cvres=$ymatho-$yhatho; $cvrest=$cvres->transpose;
  $tem=$cvrest*$cvres; 
  $cvrmse=sqrt(($tem->element(1,1))/$nho);
  @resho=NULL;
  for($i=0;$i<$nho;$i++) { $resho[$i]=$cvres->element($i+1,1); }
  @out=($n,$k,$ybar,$sse,$sstot,$residSD,$R2,$R2adj,@b,@seb,@xtxivec,
    $nho,@resho,$cvrmse);
  @out;
}

$ssize=50;
#$ssize=59;
@iperm=isamp(59,59);
@ii=@iperm[0..($ssize-1)];
@i2=@iperm[$ssize..58];
@x1=@ffarea[@ii];
@x2=@age[@ii]; @x3=@mfee[@ii]; @x4=@beds[@ii];
@x5=@baths[@ii]; @y=@askpr[@ii];
@x1ho=@ffarea[@i2];
@x2ho=@age[@i2]; @x3ho=@mfee[@i2]; @x4ho=@beds[@i2];
@x5ho=@baths[@i2]; @yho=@askpr[@i2];

$n=$ssize;
$x1str="ffarea=c(";
for($i=0;$i<$n-1;$i++) { $x1str=$x1str . $x1[$i] . ", " ; }
$x1str=$x1str . $x1[$n-1] . ")";
$x2str="age=c(";
for($i=0;$i<$n-1;$i++) { $x2str=$x2str . $x2[$i] . ", " ; }
$x2str=$x2str . $x2[$n-1] . ")";
$x3str="mfee=c(";
for($i=0;$i<$n-1;$i++) { $x3str=$x3str . $x3[$i] . ", " ; }
$x3str=$x3str . $x3[$n-1] . ")";
$x4str="beds=c(";
for($i=0;$i<$n-1;$i++) { $x4str=$x4str . $x4[$i] . ", " ; }
$x4str=$x4str . $x4[$n-1] . ")";
$x5str="baths=c(";
for($i=0;$i<$n-1;$i++) { $x5str=$x5str . $x5[$i] . ", " ; }
$x5str=$x5str . $x5[$n-1] . ")";
$ystr="askpr=c(";
for($i=0;$i<$n-1;$i++) { $ystr=$ystr . $y[$i] . ", " ; }
$ystr=$ystr . $y[$n-1] . ")";

$n2=59-$ssize;
$x1hostr="ffarea.ho=c(";
for($i=0;$i<$n2-1;$i++) { $x1hostr=$x1hostr . $x1ho[$i] . ", " ; }
$x1hostr=$x1hostr . $x1ho[$n2-1] . ")";
$x2hostr="age.ho=c(";
for($i=0;$i<$n2-1;$i++) { $x2hostr=$x2hostr . $x2ho[$i] . ", " ; }
$x2hostr=$x2hostr . $x2ho[$n2-1] . ")";
$x3hostr="mfee.ho=c(";
for($i=0;$i<$n2-1;$i++) { $x3hostr=$x3hostr . $x3ho[$i] . ", " ; }
$x3hostr=$x3hostr . $x3ho[$n2-1] . ")";
$x4hostr="beds.ho=c(";
for($i=0;$i<$n2-1;$i++) { $x4hostr=$x4hostr . $x4ho[$i] . ", " ; }
$x4hostr=$x4hostr . $x4ho[$n2-1] . ")";
$x5hostr="baths.ho=c(";
for($i=0;$i<$n2-1;$i++) { $x5hostr=$x5hostr . $x5ho[$i] . ", " ; }
$x5hostr=$x5hostr . $x5ho[$n2-1] . ")";
$yhostr="askpr.ho=c(";
for($i=0;$i<$n2-1;$i++) { $yhostr=$yhostr . $yho[$i] . ", " ; }
$yhostr=$yhostr . $yho[$n2-1] . ")";

# Cp = SS(Res)/MS(Res:full) + 2*ncol(Xmat) - n

# p=4 with beds
$Xt=Matrix ([@x0],[@x1],[@x2],[@x3],[@x4]); # 5xn matrix
$Xtho=Matrix ([@x0ho],[@x1ho],[@x2ho],[@x3ho],[@x4ho]); # 5xn2 matrix
@out4a=multregcvrmse;
@out4b=multregcvrmse2;
$n=$out4a[0]; $k=$out4a[1];
$sse4a=$out4a[3]; $residSD4=$out4a[5];
$Rsq4a=$out4a[6]; $Rsqadj4a=$out4a[7];
$nna=$#out4a; $cvrms4a=$out4a[$nna];
$nnb=$#out4b; $cvrms4b=$out4b[$nnb];


# p=3
$Xt=Matrix ([@x0],[@x1],[@x2],[@x3]); # 4xn matrix
$Xtho=Matrix ([@x0ho],[@x1ho],[@x2ho],[@x3ho]); # 4xn2 matrix
@out3a=multregcvrmse;
@out3b=multregcvrmse2;
$n=$out3a[0]; $k=$out3a[1];
$sse3a=$out3a[3]; $residSD3=$out3a[5];
$Rsq3a=$out3a[6]; $Rsqadj3a=$out3a[7];
$nna=$#out3a; $cvrms3a=$out3a[$nna];
$nnb=$#out3b; $cvrms3b=$out3b[$nnb];

# answers
$ansa1=$residSD3; 
$ansa2=$residSD4; 
$ansb1=$cvrms3a; 
$ansb2=$cvrms4a; 
$ansc1=$cvrms3b; 
$ansc2=$cvrms4b; 
$ansd="Yes";
$adif=$ansa1-$ansa2;
$bdif=$ansb1-$ansb2;
$cdif=$ansc1-$ansc2;
if($adif*$bdif<0 || $adif*$cdif<0) { $ansd="No"; }

#$anse for multiple choice to add

$smallcvrms=1;
if($ansa1< $ansb1 && $ansa1< $ansc1 && $ansa2< $ansb2 && $ansa2< $ansc2)
{ $smallcvrms=0; }

@anse=("the RMS prediction error values were not always larger for my data set",
"sampling variability",
"sample size is small",
"out-of-sample variability is always larger",
"the linear approximation to a non-linear function of the explanatory variables");

$mcquese =  new_multiple_choice();
if($smallcvrms==0)
{ $mcquese ->qa("What is the best explanation for the 
root mean square prediction errors to be larger for parts (b) and
(c)?", $anse[4]);
  $mcquese->extra($anse[0],$anse[1],$anse[2],$anse[3]);
  $ansee=$anse[4];
}
else
{ $mcquese ->qa("What is the best explanation for the 
root mean square prediction errors to be larger for parts (b) and
(c)?", $anse[0]);
  $mcquese->extra($anse[1],$anse[2],$anse[3],$anse[4]);
  $ansee=$anse[0];
}

#$BR
#residSD for models with 3 and 4 explanatory:
#$residSD3, $residSD4
#$BR
#$BR
#CVRMSE leave-one-out for  3 and 4 explanatory:
#$cvrms3a, $cvrms4a
#$BR
#$BR
#RMSE training/holdout for  3 and 4 explanatory:
#$cvrms3b, $cvrms4b
#$BR
#$BR

Context()->texStrings;

BEGIN_TEXT

The questions involves the data set for Richmond townhouses
obtained on 2014.11.03. 
$BR
$BBOLD You will get a different subset than in the preceding WebWork
question.
$EBOLD
$BR
For your subset, the response variable is:
$BR
asking price divided by 10000:
$BR
$ystr
$BR
The explanatory variables are:

$BR
(i) finished floor area divided by 100
$BR
$x1str

$BR
(ii) age
$BR
$x2str

$BR
(iii) monthly maintenance fee divided by 10
$BR
$x3str

$BR
(iv) number of bedrooms
$BR
$x4str

$BR
After you have copied the above R vectors into your R session,
you can get a dataframe with 
$BR
richmondtownh=data.frame(askpr,ffarea,age,mfee,beds)
$BR
$BR

The corresponding vectors for the holdout set are: 
$BR
$yhostr
$BR
$x1hostr
$BR
$x2hostr
$BR
$x3hostr
$BR
$x4hostr
$BR
$BR
Create a second data frame:
$BR
holdout=data.frame(askpr.ho,ffarea.ho,age.ho,mfee.ho,beds.ho)
$BR
names(holdout)=names(richmondtownh)  [to make variables names the same as before]
$BR

$BR
$BR
Use the ls.cvrmse() function from the course web site
with 3 and 4 explanatory variables, when
askpr is the response variable.
$BR
For 4 explanatory variables, all of the above are included.
$BR
For 3 explanatory variables, all of the above are included except 
$BITALIC beds $EITALIC.
$BR
 
$BR

Please use 3 decimal places for the answers below which are not integer-valued
$BR


$BR
$BBOLD Part a) $EBOLD
$BR
The values of residual SD or residual SE for the regressions with
3 and 4 explanatory variables are respectively:
$BR
3 explanatory: \{ ans_rule(8) \}
$BR
4 explanatory: \{ ans_rule(8) \}
$BR

$BR
$BBOLD Part b) $EBOLD
$BR
The values of the leave-one-out cross-validation RMS prediction error for
the regressions with 3 and 4 explanatory variables are respectively:
$BR
3 explanatory: \{ ans_rule(8) \}
$BR
4 explanatory: \{ ans_rule(8) \}
$BR

$BR
$BBOLD Part c) $EBOLD
$BR
The values of the holdout RMS prediction error for
the regressions with 3 and 4 explanatory variables are respectively:
$BR
3 explanatory: \{ ans_rule(8) \}
$BR
4 explanatory: \{ ans_rule(8) \}
$BR


$BR
$BBOLD Part d) $EBOLD
$BR
Do you get the same conclusion on which is better between
the 3-explanatory versus
4-explanatory models from the summaries in (a), (b), (c).
$BR
\{ ans_rule(8) \} (enter either Yes or No).
$BR

$BR
$BBOLD Part e) $EBOLD
$BR
Compare the three values for the models with 3 explanatory 
variables (respectively 4 explanatory); 
that is, values of residual SE, leave-one out root mean
square error and holdout subset root mean square error for
3 explanatory (respectively, 4 explanatory).
$BR
\{ $mcquese->print_q() \}
$BR
\{ $mcquese->print_a() \}
$BR

END_TEXT

#########################################################

BEGIN_HINT
Check the course pack, Section 4.2 
END_HINT

#########################################################

#########################################################
$showPartialCorrectAnswers = 1;
#########################################################

# relative default is 0.1 for .1 percent of from .999 to 1.001
# relative and tol=.01 didn't work on correctly??
ANS( num_cmp($ansa1,tol=> 0.010, tolType=>"absolute") );
ANS( num_cmp($ansa2,tol=> 0.010, tolType=>"absolute") );
ANS( num_cmp($ansb1,tol=> 0.010, tolType=>"absolute") );
ANS( num_cmp($ansb2,tol=> 0.010, tolType=>"absolute") );
ANS( num_cmp($ansc1,tol=> 0.010, tolType=>"absolute") );
ANS( num_cmp($ansc2,tol=> 0.010, tolType=>"absolute") );
ANS( str_cmp($ansd) );
ANS( radio_cmp($mcquese->correct_ans()) );

#########################################################
BEGIN_SOLUTION
$BR
(a) $ansa1, $ansa2
$BR
$BR
(b) $ansb1, $ansb2
$BR
$BR
(c) $ansc1, $ansc2
$BR
$BR
(d) $ansd
$BR
$BR
(e) $ansee
$BR
END_SOLUTION
#########################################################


ENDDOCUMENT();

